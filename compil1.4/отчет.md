% Лабораторная работа № 1.4 «Лексический распознаватель»
% 29 марта 2023 г.
% Григорьев Роман, ИУ9-61Б

# Цель работы
Целью данной работы является изучение использования детерминированных конечных автоматов 
с размеченными заключительными состояниями (лексических распознавателей) 
для решения задачи лексического анализа.

# Индивидуальный вариант
if, then, else, (, ), комментарии ограничены знаками {, } могут пересекать границы строк текста.

# Реализация

Лексическая структура языка — регулярные выражения для доменов:

* пробелы — непустые последовательности пробельных символов 
  (пробел, горизонтальная табуляция, маркеры конца строки);
* идентификаторы — непустые последовательности латинских букв и десятичных цифр, начинающиеся с буквы;
* целочисленные литералы — непустые последовательности десятичных цифр;
* ключевые слова — `if`, `then`, `else`, `(`, `)`
* знаки операций — +, =, +=
* строковые литералы — комментарии ограничены знаками {, } могут пересекать границы строк текста.

Граф недетерминированного распознавателя:

``` dot
digraph G {
    start -> whitespace[label = "\\s"];
    start -> digit[label = "\\d"];
    start -> ident_op_bracket[label = "("];
    start -> ident_cl_bracket[label = ")"];
    start -> ident_com_start[label = "{"];
    start -> identifier[label = "(?!ite)[a-zA-Z]"];
    start -> ident_i[label = "i"];
    start -> ident_t[label = "t"];
    start -> ident_e[label = "e"];

    // whitespace
    whitespace -> whitespace[label = "\\s"];
    whitespace[shape = doublecircle];

    // digit
    digit -> digit[label = "\\d"];
    digit[shape = doublecircle];

    // op_bracket
    ident_op_bracket[shape = doublecircle]

    // close_bracket
    ident_cl_bracket[shape = doublecircle]

    //keyword
    if
    ident_i -> keyword_if[label = "f"]
    keyword_if[shape = doublecircle]
    ident_i -> identifier[label = "[^f]"]

    //keyword
    then
    ident_t -> ident_th[label = "h"]
    ident_t -> identifier[label = "[^h]"]

    ident_th -> ident_the[label = "e"]
    ident_th -> identifier[label = "[^e]"]

    ident_the -> keyword_then[label = "n"]
    keyword_then[shape = doublecircle]
    ident_the -> identifier[label = "[^n]"]

    //keyword
    else
    ident_e -> ident_el[label = "l"]
    ident_e -> identifier[label = "[^l]"]

    ident_el -> ident_els[label = "s"]
    ident_el -> identifier[label = "[^s]"]

    ident_els -> keyword_else[label = "e"]
    keyword_else[shape = doublecircle]
    ident_els -> identifier[label = "[^e]"]

    //comments
    ident_com_start -> ident_com_start[label = "[^}]"]
    ident_com_start -> ident_com_finish[label = "}"]
    ident_com_finish[shape = doublecircle]

    //identifier
    identifier -> identifier[label = "[a-zA-Z0-9]"]
    identifier[shape = doublecircle]
}
```

Граф детерминированного распознавателя:

``` dot
digraph G {
    start -> whitespace[label = "\\s"];
    start -> digit[label = "\\d"];
    start -> ident_op_bracket[label = "("];
    start -> ident_cl_bracket[label = ")"];
    start -> ident_com_start[label = "{"];
    start -> identifier[label = "(?!ite)[a-zA-Z]"];
    start -> ident_i[label = "i"];
    start -> ident_t[label = "t"];
    start -> ident_e[label = "e"];

    // whitespace
    whitespace -> whitespace[label = "\\s"];
    whitespace[shape = doublecircle];

    // digit
    digit -> digit[label = "\\d"];
    digit[shape = doublecircle];

    // op_bracket
    ident_op_bracket[shape = doublecircle]

    // close_bracket
    ident_cl_bracket[shape = doublecircle]

    //keyword
    if
    ident_i -> keyword_if[label = "f"]
    keyword_if[shape = doublecircle]
    ident_i -> identifier[label = "[^f]"]

    //keyword
    then
    ident_t -> ident_th[label = "h"]
    ident_t -> identifier[label = "[^h]"]

    ident_th -> ident_the[label = "e"]
    ident_th -> identifier[label = "[^e]"]

    ident_the -> keyword_then[label = "n"]
    keyword_then[shape = doublecircle]
    ident_the -> identifier[label = "[^n]"]

    //keyword
    else
    ident_e -> ident_el[label = "l"]
    ident_e -> identifier[label = "[^l]"]

    ident_el -> ident_els[label = "s"]
    ident_el -> identifier[label = "[^s]"]

    ident_els -> keyword_else[label = "e"]
    keyword_else[shape = doublecircle]
    ident_els -> identifier[label = "[^e]"]

    //comments
    ident_com_start -> ident_com_start[label = "[^}]"]
    ident_com_start -> ident_com_finish[label = "}"]
    ident_com_finish[shape = doublecircle]

    //identifier
    identifier -> identifier[label = "[a-zA-Z0-9]"]
    identifier[shape = doublecircle]
}
```

Модуль token.py:

```python
class Token:
    def __init__(self, position, state, value):
        self.position = position
        self.state = state
        self.value = value
```

Модуль tokenanalyzer.py
```python
from Token import Token

class TokenAnalyzer:
    def __init__(self, states: dict):
        self.states = states

    @staticmethod
    def is_newline(buffer):
        return len(buffer) > 0 and buffer[-1] == '\n'

    def analyze_tokens(self, code):
        current_state = 'start'
        buffer = ''
        current_token_start_pos = (1, 0)
        char = code[0] if code else None
        i = 0
        j = 0
        tokens = []
        flag = 1

        while char is not None:
            next_state = self.get_next_state(char, current_state)

            if next_state is not None:

                current_state = next_state
                buffer += char
                i += 1
                j += 1

                if i < len(code):
                    char = code[i]
                else:
                    char = None
            else:
                if current_state not in ['whitespace', 'start']:
                    token = Token(current_token_start_pos, current_state, buffer.strip())
                    tokens.append(token)

                if self.is_newline(buffer):
                    current_token_start_pos = (current_token_start_pos[0] + flag, 0)
                    j = 0

                else:
                    current_token_start_pos = (current_token_start_pos[0], j)

                buffer = ''
                current_state = 'start'


        if current_state not in ['whitespace', 'start']:
            token = Token(current_token_start_pos, current_state, buffer.strip())
            tokens.append(token)

        return tokens

    def get_next_state(self, char, current_state):
        if current_state in self.states:
            for pattern, next_state in self.states[current_state].items():
                if re.fullmatch(pattern, char):
                    return next_state
        return None
```

Модуль main.py
```python
from Tokenanalyzer import TokenAnalyzer

if __name__ == "__main__":
    with open('input.txt', 'r') as file:
        lines = file.read()

    states = {
        'start': {
            r'\s': 'whitespace',
            r'\d': 'digit',
            r'\(': 'ident_op_bracket',
            r'\)': 'ident_cl_bracket',
            r'{': 'ident_com_start',
            r'i': 'ident_i',
            r't': 'ident_t',
            r'e': 'ident_e',
            r'(?!ite)[a-zA-Z]': 'identifier'
        },
        'whitespace': {
            r'\s': 'whitespace'
        },
        'digit': {
            r'\d': 'digit'
        },

        'ident_op_bracket': {
            r'\s': 'op_bracket'
        },
        'op_bracket': {},

        'ident_cl_bracket': {
            r'\s': 'cl_bracet'
        },
        'cl_bracket': {},

        'ident_com_start': {
            r'[^\}]': 'comment',
            r'\}': 'comment_finished'
        },
        'comment': {
            r'[^}]': 'comment',
            r'\}': 'comment_finished'
        },
        'comment_finished': {},

        'ident_i': {
            r'f': 'ident_if',
            r'[^f]': 'identifier'
        },
        'ident_if': {
            r'[\s]': 'keyword_if',
            r'[^\sw|(|)|{]': 'identifier'
        },
        'keyword_if': {},

        'ident_e': {
            r'l': 'ident_el',
            r'[^l]': 'identifier'
        },
        'ident_el': {
            r's': 'ident_els',
            r'[^s]': 'identifier'
        },
        'ident_els': {
            r'e': 'ident_else',
            r'[^e]': 'identifier'
        },
        'ident_else': {
            r'\s': 'keyword_else',
            r'[^\sw|(|)|{]': 'identifier'
        },
        'keyword_else': {},

        'ident_t': {
            r'h': 'ident_th',
            r'[^h]': 'identifier'
        },
        'ident_th': {
            r'e': 'ident_the',
            r'[^e]': 'identifier'
        },
        'ident_the': {
            r'n': 'ident_then',
            r'[^n]': 'identifier'
        },
        'ident_then': {
            r'\s': 'keyword_then',
            r'[^\sw|(|)|{]': 'identifier'
        },
        'keyword_then': {},

        'identifier': {
            r'[a-zA-Z0-9]': 'identifier'
        }
    }

    analyzer = TokenAnalyzer(states)
    tokens = analyzer.analyze_tokens(lines)


    print('*' * 40)
    for token in tokens:
        print(
            f"{token.state} ({token.position[0]}.{token.position[1]} - 
        {token.position[0]}.{len(token.value) + token.position[1]}) : {token.value}")
    print('*' * 40)
```

# Тестирование

Входные данные

```
ifasdfsdfg
if then else)
{sdfasdfsadf
sdfasd ) } (if) (else) {)(*&^%$#@!}
(sdfdsf)

```

Вывод на `stdout`

```
****************************************
identifier (1.0 - 1.10) : ifasdfsdfg
keyword_if (2.0 - 2.2) : if
keyword_then (2.3 - 2.7) : then
ident_else (2.8 - 2.12) : else
cl_bracet (2.12 - 2.13) : )
comment_finished (3.0 - 3.23) : {sdfasdfsadf
sdfasd ) }
ident_op_bracket (3.24 - 3.25) : (
ident_if (3.25 - 3.27) : if
cl_bracet (3.27 - 3.28) : )
ident_op_bracket (3.29 - 3.30) : (
ident_else (3.30 - 3.34) : else
cl_bracet (3.34 - 3.35) : )
comment_finished (3.36 - 3.48) : {)(*&^%$#@!}
ident_op_bracket (4.0 - 4.1) : (
identifier (4.1 - 4.7) : sdfdsf
cl_bracet (4.7 - 4.8) : )
****************************************
```

# Вывод
В результате выполнения данной лабораторной работы, я приобрел опыт
в изучении и применении детерминированных конечных
автоматов с размеченными заключительными состояниями,
также известных как лексические распознаватели, для решения
задачи лексического анализа. В рамках этой работы был разработан
лексический анализатор для обработки языка,
содержащего идентификаторы, целочисленные литералы, ключевые
слова. Кроме того, был построен граф,
представляющий лексический распознаватель для данного языка.
